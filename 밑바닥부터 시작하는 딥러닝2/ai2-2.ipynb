{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4113ddb",
   "metadata": {},
   "source": [
    "# wordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93f0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from NLTK) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from NLTK) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from NLTK) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from NLTK) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jin\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from click->NLTK) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6d2bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e48e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6188e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car = wordnet.synset('car.n.01')\n",
    "car.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f5ec88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6085395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('instrumentality.n.03'),\n",
       " Synset('container.n.01'),\n",
       " Synset('wheeled_vehicle.n.01'),\n",
       " Synset('self-propelled_vehicle.n.01'),\n",
       " Synset('motor_vehicle.n.01'),\n",
       " Synset('car.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.hypernym_paths()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae50ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "car = wordnet.synset('car.n.01')\n",
    "novel = wordnet.synset('novel.n.01')\n",
    "dog = wordnet.synset('dog.n.01')\n",
    "motorcycle = wordnet.synset('motorcycle.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4059cabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0b81a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.path_similarity(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c10878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.path_similarity(motorcycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6c475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and i say hello.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f02a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you say goodbye and i say hello .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('.',' .')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbce3486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4d0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id ={}\n",
    "id_to_word ={}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        newid = len(word_to_id)\n",
    "        word_to_id[word] = newid\n",
    "        id_to_word[newid] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45cd59a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30355a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38484ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0]\n",
      "[0 0 0 0 0 1 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "C = np.array([\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,0,1,0,1,0,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,1,0,0,0,0,1],\n",
    "    [0,0,0,0,0,1,0]\n",
    "])\n",
    "\n",
    "print(C[0])\n",
    "\n",
    "print(C[4])\n",
    "\n",
    "print(C[word_to_id['goodbye']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b53e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i  # left window_size\n",
    "            right_idx = idx + i  # right window_size\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a728bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./deep')\n",
    "from deep.common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vacab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vacab_size)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84077492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "945433ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "from deep.common.util import preprocess, create_co_matrix, cos_similarity\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vacab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vacab_size)\n",
    "\n",
    "C0 = C[word_to_id['you']]\n",
    "C1 = C[word_to_id['i']]\n",
    "\n",
    "print(cos_similarity(C0,C1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print(f\"{query}를 찾을 수 없습니다.\")\n",
    "        return\n",
    "    print(f\"[Query] : {query}\")\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "\n",
    "    #동시 발생행렬에서 1개씩 가져와 나의 쿼리와 유사도를 측정합니다\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        #print(f\"{i}: {similarity[i]}\")\n",
    "    \n",
    "    #유사도 측저한 데이터를 정렬한 것을 순차적으로 순회합니다.\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        #만약 자기 자신과 비교한 것은 생략합니다.\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        \n",
    "        #출력합니다.\n",
    "        #만약 원한다면 리스트에 담아서 반환하면 됩니다.\n",
    "        print(f\"{id_to_word[i]} : {similarity[i]}\")\n",
    "\n",
    "        count += 1\n",
    "        #최대 수를 초과하면 그만둡니다.\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "x = np. array([100, -20 , 2])\n",
    "print(x.argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e1c9752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] : you\n",
      "goodbye : 0.7071067691154799\n",
      "i : 0.7071067691154799\n",
      "hello : 0.7071067691154799\n",
      "say : 0.0\n",
      "and : 0.0\n"
     ]
    }
   ],
   "source": [
    "from deep.common.util import preprocess, create_co_matrix, cos_similarity\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vacab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vacab_size)\n",
    "\n",
    "most_similar('you',word_to_id,id_to_word,C,top=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1d5ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C 동시 발생 행렬\n",
    "# verbose 중간 로그\n",
    "# eps 0을 방지하기 위한 아주 작은 값.\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    #동시 발생 행렬과 크기가 같은 0으로 만들어진 PPMI 행렬 생성\n",
    "    M = np.zeros_like(C,dtype=np.float32)\n",
    "    # 동시 발생 행렬을 모두 더함\n",
    "    N = np.sum(C)\n",
    "    # 동시 발생 행렬에서 세로의 합계를 구함(차원은 세로 가로 깊이 순)\n",
    "    # 전체 말 뭉치에서 id에 해당하는 단어가 출연한 횟수\n",
    "    S = np.sum(C, axis=0)\n",
    "\n",
    "    #동시 발생 행렬의 크기를 구함\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    #반복수 0\n",
    "    cnt = 0\n",
    "\n",
    "    #행을 순회\n",
    "    for i in range(C.shape[0]):\n",
    "        #열을 순회\n",
    "        for j in range(C.shape[1]):\n",
    "            # i번째 단어와 j가 동시에 발생할 확률 * 말 뭉치속 단어의 수 / j단어와 i단어가 발생한 회수\n",
    "            # 이를 계산하고 Zero Devide를 방지하기 위해서 + eps\n",
    "            pmi = np.log2(C[i,j] * N / (S[j] * S[i]) + eps)\n",
    "            # 만약 관련도가 음수라면 pmi대신 0으로 전환하여\n",
    "            # i와 j간의 PPMI 관련 수치 업데이트\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total // 100 + 1) == 0 :\n",
    "                    print(f\"{100*cnt/total}% 완료\")\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241b0bd",
   "metadata": {},
   "source": [
    "S[j]: j번째 단어의 전체 출현 횟수\n",
    "\n",
    "S[i]: i번째 단어의 전체 출현 횟수\n",
    "\n",
    "N: 모든 단어 쌍의 동시 발생 총합\n",
    "\n",
    "PMI(i,j) = log₂(P(i,j) / (P(i) * P(j)))\n",
    "\n",
    "P(i) = S[i] / N (i번째 단어가 나타날 확률)\n",
    "\n",
    "P(j) = S[j] / N (j번째 단어가 나타날 확률)\n",
    "\n",
    "P(i,j) = C[i,j] / N (i와 j가 함께 나타날 확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c4a25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from deep.common.util import preprocess, create_co_matrix, cos_similarity\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vacab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vacab_size)\n",
    "\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"동시발생 행렬\")\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66074cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
